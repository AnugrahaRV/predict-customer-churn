{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Feature Engineering with Spark\n",
    "\n",
    "[Apache Spark](http://spark.apache.org) is a popular framework for distributed computed and large-data processing. It allows us to run computations in parallel either on a single machine, or distributed across a cluster of machines. In this notebook, we will run automated feature engineering in [Featuretools](https://github.com/Featuretools/featuretools) using Spark. \n",
    "\n",
    "We'll skip the Featuretools details in this notebook, but for an introduction see [this article](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219). For a comparison of manual to automated feature engineering, see [this article](https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is initializing Spark. We can use the `findspark` library to make sure that `pyspark` can find Spark in the Jupyter Notebook. This notebook assumes the Spark cluster is already running. To get started with a Spark cluster, refer to [this guide](https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# Initialize with Spark file location\n",
    "findspark.init('/usr/local/spark/')\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark \n",
    "\n",
    "A `SparkContext` is the gateway to the running Spark cluster. We can pass in a number of parameters to the `SparkContext` using a `SparkConf` object. Namely, we'll turn on logging, tell Spark to use all cores on our 3 machines, and direct Spark to the location of the master (parent) node. \n",
    "\n",
    "Adjust the parameters depending on your cluster set up. I found [this guide](https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html) to be helpful in choosing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('spark.eventLog.enabled', 'True'), ('spark.eventLog.dir', 'tmp/'), ('spark.num.executors', '1'), ('spark.executor.memory', '24g'), ('spark.executor.cores', '16'), ('spark.master', 'spark://ip-172-31-23-133.ec2.internal:7077')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Enable logging\n",
    "conf.set('spark.eventLog.enabled', True);\n",
    "conf.set('spark.eventLog.dir', 'tmp/');\n",
    "\n",
    "# Use all cores on all machines\n",
    "conf.set('spark.num.executors', 1)\n",
    "conf.set('spark.executor.memory', '24g')\n",
    "conf.set('spark.executor.cores', 16)\n",
    "\n",
    "# Set the parent\n",
    "conf.set('spark.master', 'spark://ip-172-31-23-133.ec2.internal:7077')\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Spark \n",
    "\n",
    "Before we get to the feature engineering, we want to test if our cluster is running correctly. We'll instantiate a `Spark` cluster and run a simple program that calculates the value of pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-23-133.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://ip-172-31-23-133.ec2.internal:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pi_calc</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://ip-172-31-23-133.ec2.internal:7077 appName=pi_calc>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"pi_calc\", \n",
    "                           conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14187128\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100000000\n",
    "import random\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "# Parallelize counting samples inside circle using Spark\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dashboards\n",
    "\n",
    "After starting the Spark cluster  from the command line- before running any of the code in the notebook - you can view a dashboard of the cluster at localhost:8080. This shows basic information such as the number of workers and the currently running or completed jobs.\n",
    "\n",
    "Once a `SparkContext` has been initialized, the job can be viewed at localhost:4040. This shows particular details such as the number of tasks completed and the directed acyclic graph of the operation. \n",
    "\n",
    "Using the web dashboard can be a helpful method to help debug your cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are confident the cluster is running correctly, we can move on to feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage\n",
    "\n",
    "All of the reading and writing for running with Spark will happen through S3. The partitioned files are all on s3 and we can use `pandas.read_csv` to read directly from s3. To write to s3, we use the `s3fs` library (shown a little later). \n",
    "\n",
    "### Read in Data from S3\n",
    "\n",
    "Before running this code, make sure to authenticate with Amazon Web Services from the command line to access your files in S3. Run `aws configure` and then input the appropriate information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "partition = 20\n",
    "directory = 's3://customer-churn-spark/partitions/p' + str(partition)\n",
    "cutoff_times_file = 'monthly_labels_30.csv'\n",
    "\n",
    "\n",
    "# Read in the data files\n",
    "members = pd.read_csv(f'{directory}/members.csv', \n",
    "                  parse_dates=['registration_init_time'], \n",
    "                  infer_datetime_format = True, \n",
    "                  dtype = {'gender': 'category'})\n",
    "\n",
    "trans = pd.read_csv(f'{directory}/transactions.csv',\n",
    "                   parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                    infer_datetime_format = True)\n",
    "\n",
    "logs = pd.read_csv(f'{directory}/logs.csv', parse_dates = ['date'])\n",
    "\n",
    "cutoff_times = pd.read_csv(f'{directory}/{cutoff_times_file}', parse_dates = ['cutoff_time'])\n",
    "cutoff_times = cutoff_times.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "First we'll make the set of features using a single partiton so we don't have to recalculate them for each partition. (It also is possible to load in calculated features from disk.) Again, I'm skipping the explanation for what is going on here so check out the [Featuretools documentation](https://docs.featuretools.com/) or some of the [online tutorials](https://www.featuretools.com/demos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: customers\n",
       "  Entities:\n",
       "    members [Rows: 6817, Columns: 6]\n",
       "    transactions [Rows: 23423, Columns: 13]\n",
       "    logs [Rows: 418190, Columns: 13]\n",
       "  Relationships:\n",
       "    transactions.msno -> members.msno\n",
       "    logs.msno -> members.msno"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty entityset\n",
    "es = ft.EntitySet(id = 'customers')\n",
    "\n",
    "# Add the members parent table\n",
    "es.entity_from_dataframe(entity_id='members', dataframe=members,\n",
    "                         index = 'msno', time_index = 'registration_init_time', \n",
    "                         variable_types = {'city': vtypes.Categorical, 'bd': vtypes.Categorical,\n",
    "                                           'registered_via': vtypes.Categorical})\n",
    "# Create new features in transactions\n",
    "trans['price_difference'] = trans['plan_list_price'] - trans['actual_amount_paid']\n",
    "trans['planned_daily_price'] = trans['plan_list_price'] / trans['payment_plan_days']\n",
    "trans['daily_price'] = trans['actual_amount_paid'] / trans['payment_plan_days']\n",
    "\n",
    "# Add the transactions child table\n",
    "es.entity_from_dataframe(entity_id='transactions', dataframe=trans,\n",
    "                         index = 'transactions_index', make_index = True,\n",
    "                         time_index = 'transaction_date', \n",
    "                         variable_types = {'payment_method_id': vtypes.Categorical, \n",
    "                                           'is_auto_renew': vtypes.Boolean, 'is_cancel': vtypes.Boolean})\n",
    "\n",
    "# Add transactions interesting values\n",
    "es['transactions']['is_cancel'].interesting_values = [0, 1]\n",
    "es['transactions']['is_auto_renew'].interesting_values = [0, 1]\n",
    "\n",
    "# Create new features in logs\n",
    "logs['total'] = logs[['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum(axis = 1)\n",
    "logs['percent_100'] = logs['num_100'] / logs['total']\n",
    "logs['percent_unique'] = logs['num_unq'] / logs['total']\n",
    "\n",
    "# Add the logs child table\n",
    "es.entity_from_dataframe(entity_id='logs', dataframe=logs,\n",
    "                     index = 'logs_index', make_index = True,\n",
    "                     time_index = 'date')\n",
    "\n",
    "# Add the relationships\n",
    "r_member_transactions = ft.Relationship(es['members']['msno'], es['transactions']['msno'])\n",
    "r_member_logs = ft.Relationship(es['members']['msno'], es['logs']['msno'])\n",
    "es.add_relationships([r_member_transactions, r_member_logs])\n",
    "\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Deep Feature Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we create the features, we use `ft.dfs` passing in the selected primitives and a few other parameters. We are also using `cutoff_time` which means that the features for every row are filtered based on the time when the label is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 230 features\n",
      "Elapsed: 05:40 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 344/344 chunks\n"
     ]
    }
   ],
   "source": [
    "# Specify primitives\n",
    "agg_primitives = ['sum', 'time_since_last', 'avg_time_between', 'all', 'mode', 'num_unique', 'min', 'last', \n",
    "                  'mean', 'percent_true', 'max', 'std', 'count']\n",
    "trans_primitives = ['weekend', 'cum_sum', 'day', 'month', 'diff', 'time_since_previous']\n",
    "where_primitives = ['sum', 'count', 'mean', 'percent_true', 'all', 'any']\n",
    "\n",
    "# Run deep feature synthesis\n",
    "feature_matrix, feature_defs = ft.dfs(entityset=es, target_entity='members', \n",
    "                                      cutoff_time = cutoff_times, \n",
    "                                      agg_primitives = agg_primitives,\n",
    "                                      trans_primitives = trans_primitives,\n",
    "                                      where_primitives = where_primitives,\n",
    "                                      max_depth = 2, features_only = False,\n",
    "                                      chunk_size = 100, n_jobs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features can then be saved on disk. Every time we want to make the same exact features, we can just pass in these into the `ft.calculate_feature_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save_features(feature_defs, '/data/churn/features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 230 features.\n"
     ]
    }
   ],
   "source": [
    "feature_defs = ft.load_features('/data/churn/features.txt')\n",
    "print(f'There are {len(feature_defs)} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Feature Matrix to S3 \n",
    "\n",
    "In order to save each feature matrix from a partition, we'll write it to s3. For this we can use the `s3fs` (s3 file system) Python library. We first have to authenticate with aws by loading in the credentials and then we can upload our csv much the same as we would write any csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "# Credentials\n",
    "with open('/data/credentials.txt', 'r') as f:\n",
    "    info = f.read().strip().split(',')\n",
    "    key = info[0]\n",
    "    secret = info[1]\n",
    "\n",
    "fs = s3fs.S3FileSystem(key=key, secret=secret)\n",
    "\n",
    "# S3 directory\n",
    "directory = 's3://customer-churn-spark/partitions/p' + str(partition)\n",
    "\n",
    "# Encode in order to write to s3\n",
    "bytes_to_write = feature_matrix.to_csv(None).encode()\n",
    "\n",
    "# Write to s3\n",
    "with fs.open(f'{directory}/feature_matrix.csv', 'wb') as f:\n",
    "    f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition to Feature Matrix Function\n",
    "\n",
    "This function:\n",
    "\n",
    "1. Takes in the name of a partition \n",
    "2. Reads the data from s3\n",
    "3. Creates an entityset from the data\n",
    "4. Computes the feature matrix for the partition\n",
    "5. Saves the feature matrix to s3\n",
    "\n",
    "Because all reading and writing happens through S3, we don't have to worry about disc space or about putting a copy of the data on each machine. Instead, we can simply read from and write to the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARTITIONS = 1000\n",
    "\n",
    "def partition_to_feature_matrix(partition, cutoff_times_file, feature_defs=feature_defs):\n",
    "    \"\"\"Take in a partition number, create a feature matrix, and save to disk\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "        partition (int): number of partition\n",
    "        cutoff_times_file (str): name of cutoff time file\n",
    "        feature_defs (list of ft features): features to make for the partition\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        None: saves the feature matrix to disk\n",
    "    \n",
    "    \"\"\"\n",
    "    directory = 's3://customer-churn-spark/partitions/p' + str(partition)\n",
    "    \n",
    "    # Read in the data files\n",
    "    members = pd.read_csv(f'{directory}/members.csv', \n",
    "                      parse_dates=['registration_init_time'], \n",
    "                      infer_datetime_format = True, \n",
    "                      dtype = {'gender': 'category'})\n",
    "\n",
    "    trans = pd.read_csv(f'{directory}/transactions.csv',\n",
    "                       parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                        infer_datetime_format = True)\n",
    "\n",
    "    logs = pd.read_csv(f'{directory}/logs.csv', parse_dates = ['date'])\n",
    "    \n",
    "    cutoff_times = pd.read_csv(f'{directory}/{cutoff_times_file}', parse_dates = ['cutoff_time'])\n",
    "    cutoff_times = cutoff_times.drop_duplicates()\n",
    "    \n",
    "    labeled_customers = set(cutoff_times['msno'])\n",
    "    \n",
    "    # Subset to only customers with labels\n",
    "    members = members[members['msno'].isin(labeled_customers)]\n",
    "    trans = trans[trans['msno'].isin(labeled_customers)]\n",
    "    logs = logs[logs['msno'].isin(labeled_customers)]\n",
    "    \n",
    "    # Create empty entityset\n",
    "    es = ft.EntitySet(id = 'customers')\n",
    "\n",
    "    # Add the members parent table\n",
    "    es.entity_from_dataframe(entity_id='members', dataframe=members,\n",
    "                             index = 'msno', time_index = 'registration_init_time', \n",
    "                             variable_types = {'city': vtypes.Categorical, 'bd': vtypes.Categorical,\n",
    "                                               'registered_via': vtypes.Categorical})\n",
    "    # Create new features in transactions\n",
    "    trans['price_difference'] = trans['plan_list_price'] - trans['actual_amount_paid']\n",
    "    trans['planned_daily_price'] = trans['plan_list_price'] / trans['payment_plan_days']\n",
    "    trans['daily_price'] = trans['actual_amount_paid'] / trans['payment_plan_days']\n",
    "\n",
    "    # Add the transactions child table\n",
    "    es.entity_from_dataframe(entity_id='transactions', dataframe=trans,\n",
    "                             index = 'transactions_index', make_index = True,\n",
    "                             time_index = 'transaction_date', \n",
    "                             variable_types = {'payment_method_id': vtypes.Categorical, \n",
    "                                               'is_auto_renew': vtypes.Boolean, 'is_cancel': vtypes.Boolean})\n",
    "\n",
    "    # Add transactions interesting values\n",
    "    es['transactions']['is_cancel'].interesting_values = [0, 1]\n",
    "    es['transactions']['is_auto_renew'].interesting_values = [0, 1]\n",
    "    \n",
    "    # Create new features in logs\n",
    "    logs['total'] = logs[['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum(axis = 1)\n",
    "    logs['percent_100'] = logs['num_100'] / logs['total']\n",
    "    logs['percent_unique'] = logs['num_unq'] / logs['total']\n",
    "    \n",
    "    # Add the logs child table\n",
    "    es.entity_from_dataframe(entity_id='logs', dataframe=logs,\n",
    "                         index = 'logs_index', make_index = True,\n",
    "                         time_index = 'date')\n",
    "\n",
    "    # Add the relationships\n",
    "    r_member_transactions = ft.Relationship(es['members']['msno'], es['transactions']['msno'])\n",
    "    r_member_logs = ft.Relationship(es['members']['msno'], es['logs']['msno'])\n",
    "    es.add_relationships([r_member_transactions, r_member_logs])\n",
    "\n",
    "    # Calculate and save the feature matrix\n",
    "    feature_matrix = ft.calculate_feature_matrix(entityset=es, \n",
    "                                                 features=feature_defs, \n",
    "                                                 cutoff_time=cutoff_times,\n",
    "                                                 chunk_size = len(es['members'].df))\n",
    "    \n",
    "    # Encode in order to write to s3\n",
    "    bytes_to_write = feature_matrix.to_csv(None).encode()\n",
    "    \n",
    "    # Write to s3\n",
    "    with fs.open(f'{directory}/feature_matrix.csv', 'wb') as f:\n",
    "        f.write(bytes_to_write)\n",
    "    \n",
    "    # Report progress every 10th of number of partitions\n",
    "    if (partition % (N_PARTITIONS / 10) == 0):\n",
    "        print(f'{100 * round(partition / N_PARTITIONS)}% complete.', end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function\n",
    "\n",
    "Let's give the function a test with 2 different partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "partition_to_feature_matrix(950, 'monthly_labels_30.csv', feature_defs)\n",
    "end = timer()\n",
    "print(f'{round(end - start)} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "partition_to_feature_matrix(530, 'monthly_labels_30.csv', feature_defs)\n",
    "end = timer()\n",
    "print(f'{round(end - start)} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>gender</th>\n",
       "      <th>SUM(logs.num_25)</th>\n",
       "      <th>SUM(logs.num_50)</th>\n",
       "      <th>SUM(logs.num_75)</th>\n",
       "      <th>SUM(logs.num_985)</th>\n",
       "      <th>SUM(logs.num_100)</th>\n",
       "      <th>SUM(logs.num_unq)</th>\n",
       "      <th>...</th>\n",
       "      <th>WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>DAY(LAST(logs.date))</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>DAY(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>churn</th>\n",
       "      <th>days_to_next_churn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msno</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=</th>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=</th>\n",
       "      <td>5.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              city    bd  registered_via  \\\n",
       "msno                                                                       \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=  13.0  26.0             9.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=  22.0   0.0             3.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=   1.0   0.0             7.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=  13.0   0.0             9.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=   5.0  55.0             9.0   \n",
       "\n",
       "                                              gender  SUM(logs.num_25)  \\\n",
       "msno                                                                     \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=    male               0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=     NaN               0.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=     NaN               0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=     NaN               2.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=  female               4.0   \n",
       "\n",
       "                                              SUM(logs.num_50)  \\\n",
       "msno                                                             \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=               0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=               1.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=               0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=               0.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=               3.0   \n",
       "\n",
       "                                              SUM(logs.num_75)  \\\n",
       "msno                                                             \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=               0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=               0.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=               0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=               0.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=               2.0   \n",
       "\n",
       "                                              SUM(logs.num_985)  \\\n",
       "msno                                                              \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                0.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                2.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                5.0   \n",
       "\n",
       "                                              SUM(logs.num_100)  \\\n",
       "msno                                                              \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=               12.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                8.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=               20.0   \n",
       "\n",
       "                                              SUM(logs.num_unq)  \\\n",
       "msno                                                              \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                2.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=               12.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=               12.0   \n",
       "\n",
       "                                                     ...          \\\n",
       "msno                                                 ...           \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=         ...           \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=         ...           \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=         ...           \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=         ...           \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=         ...           \n",
       "\n",
       "                                              WEEKEND(LAST(transactions.transaction_date))  \\\n",
       "msno                                                                                         \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                           0.0   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                           0.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                           0.0   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                           0.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                           0.0   \n",
       "\n",
       "                                              WEEKEND(LAST(transactions.membership_expire_date))  \\\n",
       "msno                                                                                               \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                                0.0    \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                                0.0    \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                                0.0    \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                                0.0    \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                                0.0    \n",
       "\n",
       "                                              DAY(LAST(logs.date))  \\\n",
       "msno                                                                 \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                   NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                   1.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                   NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                   1.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                   1.0   \n",
       "\n",
       "                                              DAY(LAST(transactions.transaction_date))  \\\n",
       "msno                                                                                     \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                       NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                       NaN   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                       NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                       NaN   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                       NaN   \n",
       "\n",
       "                                              DAY(LAST(transactions.membership_expire_date))  \\\n",
       "msno                                                                                           \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                             NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                             NaN   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                             NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                             NaN   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                             NaN   \n",
       "\n",
       "                                              MONTH(LAST(logs.date))  \\\n",
       "msno                                                                   \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                     NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                     1.0   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                     NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                     1.0   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                     1.0   \n",
       "\n",
       "                                              MONTH(LAST(transactions.transaction_date))  \\\n",
       "msno                                                                                       \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                         NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                         NaN   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                         NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                         NaN   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                         NaN   \n",
       "\n",
       "                                              MONTH(LAST(transactions.membership_expire_date))  \\\n",
       "msno                                                                                             \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=                                               NaN   \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=                                               NaN   \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=                                               NaN   \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=                                               NaN   \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=                                               NaN   \n",
       "\n",
       "                                              churn  days_to_next_churn  \n",
       "msno                                                                     \n",
       "+9+/k7BKiM5RS+cndOxiH/bParrWtz7JOGSyfiq5D2I=    0.0               458.0  \n",
       "+Me2HZ/VTA3gqYrRDIgEpKBrv7ndFXu/Za/6NrCL4s4=    0.0                 NaN  \n",
       "+NDui5w0wAj0vG8VSE5dMrwGbSC0os5IzuM1ypHR2ks=    0.0                 NaN  \n",
       "+TKjVbDcftfMZmuwXRCMJBUh90d06pfNp5N75jk1CNU=    0.0                 NaN  \n",
       "+TkpIsqRe7PZUqdwkDExEaQIc6XQvWNOVhscxWlaNoQ=    0.0               477.0  \n",
       "\n",
       "[5 rows x 232 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with Spark\n",
    "\n",
    "The next cell parallelizes the feature engineering calculations using Spark. We want to `map` the partitions to the function and we let Spark divide the work between the executors. At the end of the computation, all of the files will be uploaded to S3 in the correct partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of partitions\n",
    "partitions = list(range(N_PARTITIONS))\n",
    "\n",
    "# Create Spark context\n",
    "sc = pyspark.SparkContext(master = 'spark://ip-172-31-23-133.ec2.internal:7077',\n",
    "                          appName = 'featuretools', conf = conf)\n",
    "\n",
    "# Parallelize feature engineering\n",
    "r = sc.parallelize(partitions, numSlices=N_PARTITIONS).\\\n",
    "    map(lambda x: partition_to_feature_matrix(x, 'monthly_labels_30.csv',\n",
    "                                               feature_defs)).collect()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the run is going on, we can look at the status of the cluster at localhost:8080 and the state of the particular job at localhost:4040. \n",
    "\n",
    "__Here is the overall state of the cluster.__\n",
    "\n",
    "![](../images/spark_cluster2.png)\n",
    "\n",
    "__Here is information about the submitted job.__\n",
    "\n",
    "![](../images/spark_job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "From here, we could read in all the partitioned feature matrices and build a single feature matrix or if we have a model that supports incremental (also known as on-line) learning, we can train it with one partition at a time. One of the benefits of storing our data on S3 is we can now access it from any machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (76,77,119,124,153,156,160,161,165,174,175) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>gender</th>\n",
       "      <th>SUM(logs.num_25)</th>\n",
       "      <th>SUM(logs.num_50)</th>\n",
       "      <th>SUM(logs.num_75)</th>\n",
       "      <th>SUM(logs.num_985)</th>\n",
       "      <th>SUM(logs.num_100)</th>\n",
       "      <th>...</th>\n",
       "      <th>WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>DAY(LAST(logs.date))</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>DAY(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>churn</th>\n",
       "      <th>days_to_next_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+9v4Rbyc+58MyKbt1wrCskWClJadOJh7CapZa9CYXUM=</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+FMjiiorqZQ3ZzNNmgO0vZM2yh8IHPvWSvwy2fSBMLU=</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+V3HOZsK34UPrNOYg6IhG8sP1dY6w5LG8J98eodnBBk=</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+ikgRAmrCW349x39kQ0nOqh9jvajPXJFZkI9Q6omEMs=</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+kbXNszLheADYStfNoRwa9q9sZykS5Tfk044GMwOw1o=</td>\n",
       "      <td>15.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 233 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  city    bd  registered_via  \\\n",
       "0  +9v4Rbyc+58MyKbt1wrCskWClJadOJh7CapZa9CYXUM=   5.0  24.0             7.0   \n",
       "1  +FMjiiorqZQ3ZzNNmgO0vZM2yh8IHPvWSvwy2fSBMLU=   6.0  27.0             7.0   \n",
       "2  +V3HOZsK34UPrNOYg6IhG8sP1dY6w5LG8J98eodnBBk=   NaN   NaN             NaN   \n",
       "3  +ikgRAmrCW349x39kQ0nOqh9jvajPXJFZkI9Q6omEMs=  14.0   0.0             9.0   \n",
       "4  +kbXNszLheADYStfNoRwa9q9sZykS5Tfk044GMwOw1o=  15.0  29.0             9.0   \n",
       "\n",
       "   gender  SUM(logs.num_25)  SUM(logs.num_50)  SUM(logs.num_75)  \\\n",
       "0  female               0.0               0.0               0.0   \n",
       "1    male               0.0               0.0               0.0   \n",
       "2     NaN               0.0               0.0               0.0   \n",
       "3     NaN               0.0               0.0               0.0   \n",
       "4    male              22.0               4.0               5.0   \n",
       "\n",
       "   SUM(logs.num_985)  SUM(logs.num_100)         ...          \\\n",
       "0                0.0                0.0         ...           \n",
       "1                0.0              111.0         ...           \n",
       "2                0.0                0.0         ...           \n",
       "3                0.0                0.0         ...           \n",
       "4                3.0               54.0         ...           \n",
       "\n",
       "   WEEKEND(LAST(transactions.transaction_date))  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           NaN   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   WEEKEND(LAST(transactions.membership_expire_date))  DAY(LAST(logs.date))  \\\n",
       "0                                                0.0                    NaN   \n",
       "1                                                0.0                    1.0   \n",
       "2                                                NaN                    NaN   \n",
       "3                                                0.0                    NaN   \n",
       "4                                                0.0                    1.0   \n",
       "\n",
       "   DAY(LAST(transactions.transaction_date))  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   DAY(LAST(transactions.membership_expire_date))  MONTH(LAST(logs.date))  \\\n",
       "0                                             NaN                     NaN   \n",
       "1                                             NaN                     1.0   \n",
       "2                                             NaN                     NaN   \n",
       "3                                             NaN                     NaN   \n",
       "4                                             NaN                     1.0   \n",
       "\n",
       "   MONTH(LAST(transactions.transaction_date))  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "\n",
       "   MONTH(LAST(transactions.membership_expire_date))  churn  days_to_next_churn  \n",
       "0                                               NaN    0.0                 NaN  \n",
       "1                                               NaN    0.0                 NaN  \n",
       "2                                               NaN    0.0                 NaN  \n",
       "3                                               NaN    0.0               462.0  \n",
       "4                                               NaN    0.0                 NaN  \n",
       "\n",
       "[5 rows x 233 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv('s3://customer-churn-spark/partitions/p50/feature_matrix.csv')\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to distribute feature engineering in Featuretools using the Spark framework. This big-data processing technology lets us use multiple computers to parallelize calculations, resulting in efficient data science workflows even on large datasets. Moreover, we saw how the same partition and distribute approach that worked with Dask can also work with Spark. The nice part about these frameworks is we don't have to change the underlying Featuretools code. We simply write our code in native Python, change the backend running the calculations, and distribute the calculations across a cluster of machines. Using this approach, we'll be able to scale to any size datasets and take on even more exciting data science and machine learning problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

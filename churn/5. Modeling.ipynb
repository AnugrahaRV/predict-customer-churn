{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Modeling\n",
    "\n",
    "The last step in the machine learning pipeline is also where the value is derived. After we have developed the labels, each with its own cutoff time, we need to train a model to map from the features to predict the label. \n",
    "\n",
    "In this notebook, we will use the feature matrices and label times to train and test a machine learning model. We will work through a single prediction problem, predicting on the first of each month which customers will churn during the month. \n",
    "\n",
    "## Approach\n",
    "\n",
    "Our basic machine learning approach is:\n",
    "\n",
    "1. Prepare data for machine learning\n",
    "    * Fill in missing values with median imputation\n",
    "    * Encoding of categorical values\n",
    "2. Split data into training and hold out testing based on time \n",
    "3. Evaluate a baseline logistic regression model\n",
    "    * Also try a naive baseline for comparison\n",
    "4. Try a non-linear more capable classifier, the Random Forest \n",
    "    * Use mostly default hyperparameters\n",
    "    * Evaluate on hold-out testing data\n",
    "5. Inspect predictions to determine if business need has been met\n",
    "    * Precision recall curve used to tune threshold\n",
    "    * Confusion matrix to assess predictions\n",
    "6. Optimizer model automatically using an auto-ml library\n",
    "    * Using TPOT although many options exist\n",
    "    \n",
    "The final outcome is an optimized model that solves the business problem of predicting customer churn with given parameters. The model can be deployed - used to make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:57:03.774931Z",
     "start_time": "2018-10-24T00:57:03.412366Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PARTITION_DIR = 's3://customer-churn-spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:57:06.172871Z",
     "start_time": "2018-10-24T00:57:03.776538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>time</th>\n",
       "      <th>bd</th>\n",
       "      <th>city</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>gender</th>\n",
       "      <th>SUM(logs.num_25)</th>\n",
       "      <th>SUM(logs.num_50)</th>\n",
       "      <th>SUM(logs.num_75)</th>\n",
       "      <th>SUM(logs.num_985)</th>\n",
       "      <th>...</th>\n",
       "      <th>WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>DAY(LAST(logs.date))</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>DAY(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>label</th>\n",
       "      <th>days_to_churn</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+9zx0+mA3IZQLyjmU88qbfqJ0q9okIfYZnDI6FqaN2o=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+sZCvwt5NmFw4uE185pBid4cOxtXTHovIyPFqchulQg=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>395.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+wzmLe86mMBeoIYoPedlt24WVTW6tabsRcaz81ZXBx0=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/9+HJnqEryBbuH598zKqa8zb1Eypy927imqI9IWhJTk=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>488.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/BAK3DkUpoUESh4t8qlWs16yop+sG3i3oPYDpv5uGI0=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno        time    bd  city  \\\n",
       "0  +9zx0+mA3IZQLyjmU88qbfqJ0q9okIfYZnDI6FqaN2o=  2015-01-01   0.0   1.0   \n",
       "1  +sZCvwt5NmFw4uE185pBid4cOxtXTHovIyPFqchulQg=  2015-01-01   0.0   1.0   \n",
       "2  +wzmLe86mMBeoIYoPedlt24WVTW6tabsRcaz81ZXBx0=  2015-01-01   0.0   1.0   \n",
       "3  /9+HJnqEryBbuH598zKqa8zb1Eypy927imqI9IWhJTk=  2015-01-01  29.0   8.0   \n",
       "4  /BAK3DkUpoUESh4t8qlWs16yop+sG3i3oPYDpv5uGI0=  2015-01-01  21.0  13.0   \n",
       "\n",
       "   registered_via gender  SUM(logs.num_25)  SUM(logs.num_50)  \\\n",
       "0             7.0    NaN               0.0               0.0   \n",
       "1             7.0    NaN               0.0               0.0   \n",
       "2             7.0    NaN               0.0               0.0   \n",
       "3             9.0   male               3.0               0.0   \n",
       "4             9.0   male               0.0               0.0   \n",
       "\n",
       "   SUM(logs.num_75)  SUM(logs.num_985)     ...      \\\n",
       "0               0.0                0.0     ...       \n",
       "1               0.0                0.0     ...       \n",
       "2               0.0                0.0     ...       \n",
       "3               0.0                0.0     ...       \n",
       "4               0.0                0.0     ...       \n",
       "\n",
       "   WEEKEND(LAST(transactions.membership_expire_date))  DAY(LAST(logs.date))  \\\n",
       "0                                                0.0                    NaN   \n",
       "1                                                1.0                    NaN   \n",
       "2                                                0.0                    NaN   \n",
       "3                                                0.0                    1.0   \n",
       "4                                                0.0                    NaN   \n",
       "\n",
       "   DAY(LAST(transactions.transaction_date))  \\\n",
       "0                                       NaN   \n",
       "1                                       1.0   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   DAY(LAST(transactions.membership_expire_date))  MONTH(LAST(logs.date))  \\\n",
       "0                                             NaN                     NaN   \n",
       "1                                             1.0                     NaN   \n",
       "2                                             NaN                     NaN   \n",
       "3                                             NaN                     1.0   \n",
       "4                                             NaN                     NaN   \n",
       "\n",
       "   MONTH(LAST(transactions.transaction_date))  \\\n",
       "0                                         NaN   \n",
       "1                                         1.0   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "\n",
       "   MONTH(LAST(transactions.membership_expire_date))  label  days_to_churn  \\\n",
       "0                                               NaN    0.0            NaN   \n",
       "1                                               2.0    NaN          395.0   \n",
       "2                                               NaN    0.0            NaN   \n",
       "3                                               NaN    NaN          488.0   \n",
       "4                                               NaN    0.0            NaN   \n",
       "\n",
       "   churn_date  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0_fm = pd.read_csv(f'{PARTITION_DIR}/p0/MS-30_feature_matrix.csv')\n",
    "p0_fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data\n",
    "\n",
    "All of the data is stored in AWS S3. We'll retrieve 50 partitions of feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:57:06.176515Z",
     "start_time": "2018-10-24T00:57:06.174309Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_data(partition_num, label_type='MS-30'):\n",
    "    return pd.read_csv(f'{PARTITION_DIR}/p{partition_num}/{label_type}_feature_matrix.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell retrieves a number of feature matrices for training and for testing. This is done in parallel using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:57:32.773254Z",
     "start_time": "2018-10-24T00:57:06.177996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0% complete.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1301613, 260)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(50)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(6)\n",
    "\n",
    "# Set number of train and testing feature matrices\n",
    "fms_to_get = 50\n",
    "\n",
    "# Choose random sample of partitions\n",
    "ps = random.sample(list(range(900)), fms_to_get)\n",
    "\n",
    "# Retrieve feature matrices from S3\n",
    "fms = []\n",
    "for i, r in enumerate(pool.imap_unordered(retrieve_data, ps)):\n",
    "    print(f'{round(100 * (i / fms_to_get), 2)}% complete.', end='\\r')\n",
    "    fms.append(r)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Join together and drop rows with unknown label\n",
    "feature_matrix = pd.concat(fms)\n",
    "feature_matrix = feature_matrix[~feature_matrix['label'].isna()]\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below converts the boolean types to integers for use in a machine learning model. Most of the boolean indicate whether or not all the values for customer were True (`ALL` primitive) or if the date was a weekend (`WEEKEND` primitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:58:00.307302Z",
     "start_time": "2018-10-24T00:57:32.774826Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix.replace({'False': 0, 'True': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:58:07.637872Z",
     "start_time": "2018-10-24T00:58:00.308856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL(logs.WEEKEND(date))</th>\n",
       "      <th>ALL(transactions.WEEKEND(membership_expire_date) WHERE is_auto_renew = 0)</th>\n",
       "      <th>ALL(transactions.WEEKEND(membership_expire_date) WHERE is_auto_renew = 1)</th>\n",
       "      <th>ALL(transactions.WEEKEND(membership_expire_date) WHERE is_cancel = 0)</th>\n",
       "      <th>ALL(transactions.WEEKEND(membership_expire_date) WHERE is_cancel = 1)</th>\n",
       "      <th>ALL(transactions.WEEKEND(membership_expire_date))</th>\n",
       "      <th>ALL(transactions.WEEKEND(transaction_date) WHERE is_auto_renew = 0)</th>\n",
       "      <th>ALL(transactions.WEEKEND(transaction_date) WHERE is_auto_renew = 1)</th>\n",
       "      <th>ALL(transactions.WEEKEND(transaction_date) WHERE is_cancel = 0)</th>\n",
       "      <th>ALL(transactions.WEEKEND(transaction_date) WHERE is_cancel = 1)</th>\n",
       "      <th>...</th>\n",
       "      <th>ALL(transactions.is_cancel WHERE is_auto_renew = 0)</th>\n",
       "      <th>ALL(transactions.is_cancel WHERE is_auto_renew = 1)</th>\n",
       "      <th>ALL(transactions.is_cancel)</th>\n",
       "      <th>LAST(logs.WEEKEND(date))</th>\n",
       "      <th>LAST(transactions.WEEKEND(membership_expire_date))</th>\n",
       "      <th>LAST(transactions.WEEKEND(transaction_date))</th>\n",
       "      <th>WEEKEND(LAST(logs.date))</th>\n",
       "      <th>WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>WEEKEND(registration_init_time)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ALL(logs.WEEKEND(date))  \\\n",
       "0                      NaN   \n",
       "1                      0.0   \n",
       "2                      0.0   \n",
       "3                      0.0   \n",
       "4                      NaN   \n",
       "\n",
       "   ALL(transactions.WEEKEND(membership_expire_date) WHERE is_auto_renew = 0)  \\\n",
       "0                                                NaN                           \n",
       "1                                                NaN                           \n",
       "2                                                NaN                           \n",
       "3                                                NaN                           \n",
       "4                                                NaN                           \n",
       "\n",
       "   ALL(transactions.WEEKEND(membership_expire_date) WHERE is_auto_renew = 1)  \\\n",
       "0                                                NaN                           \n",
       "1                                                NaN                           \n",
       "2                                                NaN                           \n",
       "3                                                NaN                           \n",
       "4                                                0.0                           \n",
       "\n",
       "   ALL(transactions.WEEKEND(membership_expire_date) WHERE is_cancel = 0)  \\\n",
       "0                                                NaN                       \n",
       "1                                                NaN                       \n",
       "2                                                NaN                       \n",
       "3                                                NaN                       \n",
       "4                                                0.0                       \n",
       "\n",
       "   ALL(transactions.WEEKEND(membership_expire_date) WHERE is_cancel = 1)  \\\n",
       "0                                                NaN                       \n",
       "1                                                NaN                       \n",
       "2                                                NaN                       \n",
       "3                                                NaN                       \n",
       "4                                                NaN                       \n",
       "\n",
       "   ALL(transactions.WEEKEND(membership_expire_date))  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                0.0   \n",
       "\n",
       "   ALL(transactions.WEEKEND(transaction_date) WHERE is_auto_renew = 0)  \\\n",
       "0                                                NaN                     \n",
       "1                                                NaN                     \n",
       "2                                                NaN                     \n",
       "3                                                NaN                     \n",
       "4                                                NaN                     \n",
       "\n",
       "   ALL(transactions.WEEKEND(transaction_date) WHERE is_auto_renew = 1)  \\\n",
       "0                                                NaN                     \n",
       "1                                                NaN                     \n",
       "2                                                NaN                     \n",
       "3                                                NaN                     \n",
       "4                                                0.0                     \n",
       "\n",
       "   ALL(transactions.WEEKEND(transaction_date) WHERE is_cancel = 0)  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                0.0                 \n",
       "\n",
       "   ALL(transactions.WEEKEND(transaction_date) WHERE is_cancel = 1)  \\\n",
       "0                                                NaN                 \n",
       "1                                                NaN                 \n",
       "2                                                NaN                 \n",
       "3                                                NaN                 \n",
       "4                                                NaN                 \n",
       "\n",
       "                ...                 \\\n",
       "0               ...                  \n",
       "1               ...                  \n",
       "2               ...                  \n",
       "3               ...                  \n",
       "4               ...                  \n",
       "\n",
       "   ALL(transactions.is_cancel WHERE is_auto_renew = 0)  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                                NaN     \n",
       "4                                                NaN     \n",
       "\n",
       "   ALL(transactions.is_cancel WHERE is_auto_renew = 1)  \\\n",
       "0                                                NaN     \n",
       "1                                                NaN     \n",
       "2                                                NaN     \n",
       "3                                                NaN     \n",
       "4                                                0.0     \n",
       "\n",
       "   ALL(transactions.is_cancel)  LAST(logs.WEEKEND(date))  \\\n",
       "0                          NaN                       NaN   \n",
       "1                          NaN                       0.0   \n",
       "2                          NaN                       0.0   \n",
       "3                          NaN                       0.0   \n",
       "4                          0.0                       NaN   \n",
       "\n",
       "   LAST(transactions.WEEKEND(membership_expire_date))  \\\n",
       "0                                                NaN    \n",
       "1                                                NaN    \n",
       "2                                                NaN    \n",
       "3                                                NaN    \n",
       "4                                                0.0    \n",
       "\n",
       "   LAST(transactions.WEEKEND(transaction_date))  WEEKEND(LAST(logs.date))  \\\n",
       "0                                           NaN                       0.0   \n",
       "1                                           NaN                       0.0   \n",
       "2                                           NaN                       0.0   \n",
       "3                                           NaN                       0.0   \n",
       "4                                           0.0                       0.0   \n",
       "\n",
       "   WEEKEND(LAST(transactions.membership_expire_date))  \\\n",
       "0                                                0.0    \n",
       "1                                                0.0    \n",
       "2                                                0.0    \n",
       "3                                                0.0    \n",
       "4                                                0.0    \n",
       "\n",
       "   WEEKEND(LAST(transactions.transaction_date))  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "\n",
       "   WEEKEND(registration_init_time)  \n",
       "0                              0.0  \n",
       "1                              1.0  \n",
       "2                              1.0  \n",
       "3                              0.0  \n",
       "4                              0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_original = feature_matrix.copy()\n",
    "feature_matrix.drop(columns = [c for c in ['churn', 'days_to_next_churn', \n",
    "                                           'churn_date'] if c in feature_matrix],\n",
    "                    inplace = True)\n",
    "\n",
    "bool_cols = [c for c in feature_matrix if 'ALL' in c or ('WEEKEND' in c and 'PERCENT_TRUE' not in c)]\n",
    "\n",
    "for c in bool_cols:\n",
    "    feature_matrix[c] = feature_matrix[c].astype(float)\n",
    "feature_matrix[bool_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We'll do a few basic data cleaning steps:\n",
    "\n",
    "* Remove columns with many missing values\n",
    "* Remove columns with a single unique value\n",
    "* Remove highly correlated - colinear - columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "We'll drop any columns with more than 90% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:58:13.271004Z",
     "start_time": "2018-10-24T00:58:07.639293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAST(logs.seconds_per_song)',\n",
       " 'MAX(logs.seconds_per_song)',\n",
       " 'MEAN(logs.seconds_per_song)',\n",
       " 'MIN(logs.seconds_per_song)',\n",
       " 'STD(logs.seconds_per_song)',\n",
       " 'SUM(logs.seconds_per_song)',\n",
       " 'TOTAL_PREVIOUS_MONTH(logs.seconds_per_song, date)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_pct = feature_matrix.isnull().sum() / len(feature_matrix)\n",
    "to_drop = list((missing_pct[missing_pct > 0.9]).index)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:58:14.445924Z",
     "start_time": "2018-10-24T00:58:13.272716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1301613, 252)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.drop(columns = to_drop, inplace = True)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Unique Value\n",
    "\n",
    "Columns with only a single unique value contain no information and hence can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:58:59.645476Z",
     "start_time": "2018-10-24T00:58:14.447414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALL(transactions.is_auto_renew WHERE is_cancel = 1)',\n",
       " 'ALL(transactions.is_cancel WHERE is_auto_renew = 0)',\n",
       " 'PERCENT_TRUE(transactions.is_cancel WHERE is_auto_renew = 0)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unique = feature_matrix.apply(lambda x: x.nunique() == 1, axis = 0)\n",
    "to_drop = list(one_unique[one_unique == True].index)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T00:59:00.773206Z",
     "start_time": "2018-10-24T00:58:59.646877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1301613, 249)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.drop(columns = to_drop, inplace = True)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highly Correlated (collinear) Columns\n",
    "\n",
    "Collinear columns can slow down training, lead to less interpretable models, and decrease generalization performance. Therefore, it's generally a good idea to remove one of each pair of highly correlated columns for machine learning. The following code identifies columns that exceed an absolute magnitude correlation of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.432Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.95\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = feature_matrix.corr().abs()\n",
    "\n",
    "# Subset to the upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Identify names of columns with correlation above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.436Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'There are {len(to_drop)} columns to drop with correlation > {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.439Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix.drop(columns = to_drop, inplace = True)\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data cleaning operations should increase the generalization performance of our model and make it more interpretable. A few simple operations can greatly improve a machine learning model and often are more effective than model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate into Training and Testing Set\n",
    "\n",
    "We'll separate into a training and testing set based on the date. We'll use 25% of the data in the testing and 75% in the training. Separating training and testing by the date is important in time sensitive problems because it prevents data leakage and gives a better estimate of the generalization performance of the model. On real data, our model will have to make forecasts of the future, and we can try to recreate that situation by using a hold-out set from later in time than the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.443Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix['time'] = pd.to_datetime(feature_matrix['time'])\n",
    "feature_matrix['time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below splits the data based on the time. First we sort by the time and then find the first 75% for training and the latter 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.447Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.75\n",
    "test_start = int(len(feature_matrix) * train_fraction)\n",
    "feature_matrix.sort_values('time', inplace = True)\n",
    "\n",
    "train = feature_matrix.iloc[:test_start].copy()\n",
    "test = feature_matrix.iloc[test_start:].copy()\n",
    "\n",
    "train.sort_values(['time'], inplace = True)\n",
    "test.sort_values(['time'], inplace = True)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparation\n",
    "\n",
    "The next blocks of code get the features ready for machine learning.\n",
    "\n",
    "\n",
    "### Encoding Categoricals\n",
    "\n",
    "First we need to one hot encode the features. After doing this, we align the training and testing dataframes so they have the same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.452Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train.drop(columns = ['time', 'msno']))\n",
    "test = pd.get_dummies(test.drop(columns = ['time', 'msno']))\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Labels\n",
    "\n",
    "Now we can extract the labels. There are two different problems: one is a binary classification of whether or not the customer will churn during the month. The other is a regression: how many days are there until the next churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.456Z"
    }
   },
   "outputs": [],
   "source": [
    "y, test_y = np.array(train.pop('label')), np.array(test.pop('label'))\n",
    "\n",
    "y_reg, test_y_reg = np.array(train.pop('days_to_churn')), np.array(\n",
    "    test.pop('days_to_churn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.459Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(y); plt.title('Label Distribution');\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.462Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.nan_to_num(y_reg)); plt.title('Regression Labels');\n",
    "plt.ylabel('Count'); plt.xlabel('Days to Next Churn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in Missing Values\n",
    "\n",
    "We can fill in missing values using the median of the column. As an important note, the missing test values are filled in with the median of the corresponding training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.465Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.replace({np.inf: np.nan, -np.inf: np.nan}).\\\n",
    "    fillna(train.median()).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.469Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test.replace({np.inf: np.nan, -np.inf: np.nan}).\\\n",
    "    fillna(train.median()).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.471Z"
    }
   },
   "outputs": [],
   "source": [
    "np.any(train.isnull()), np.any(np.isinf(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no missing values and all of the values are numeric, our data is ready for machine learning. However, before we do machine learning, we need to figure out what a naive baseline would score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline\n",
    "\n",
    "For a naive baseline, we can randomly guess that a customer has churned with the same frequence of the churns in the training data. We'll assess the predictions using a number of different metrics.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "For an imbalanced classification problem, there are a number of metrics to consider:\n",
    "\n",
    "* Receiver Operating Characteristic Area Under the Curve (ROC AUC): a measure between 0 and 1 comparing the performance of the classifier when predicting probabilities across a range of thresholds.\n",
    "* Precision Score: number of true positives divided by the total number of positives predicted\n",
    "* Recall Score: number of true positives divided by the total number of actual positives in the data\n",
    "* F1 Score: Harmonic mean of precision and recall\n",
    "\n",
    "The exact metric used and the threshold that our model needs to reach depends on the business need. We can tune the model to some extent to optimize for different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.476Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "naive_guess = np.random.binomial(1, p = np.mean(y), size = len(test_y))\n",
    "naive_guess[:10], naive_guess.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.480Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (roc_auc_score, precision_score, \n",
    "                             recall_score, f1_score)\n",
    "\n",
    "print(f'Naive Baseline\\n')\n",
    "roc = roc_auc_score(test_y, np.repeat(np.mean(y), len(test_y)))\n",
    "print(f'ROC AUC: {round(roc, 4)}')\n",
    "\n",
    "for metric in [precision_score, recall_score, f1_score]:\n",
    "    print(f'{metric.__name__}: {round(metric(test_y, naive_guess), 4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these metrics are very poor! With no machine learning, we are only able to identify 2.94% of the customer churns and 99.13% of our predicted churns are actually false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.484Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The percentage of churns is {100 * round(np.mean(y), 4)}% in the training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Now we need to see if machine learning is up to the task of improving on these predictions. We'll start simple, using a linear model to assess if this problem is easy enough to be solved with Logistic Regression. \n",
    "\n",
    "(The machine learning models are implemented in [Scikit-Learn](https://sklearn.org/)). \n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "We can use a logistic regression in order to see baseline performance on this problem. If the logistic regression works well enough, then there is no need to move to a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state = 50, solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a simple function to evaluate predictions. This implements the metrics used above. (A full list of metrics in Scikit-Learn can be found in [this documentation](http://scikit-learn.org/stable/modules/classes.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate(model, train, y, test, test_y):\n",
    "    \"\"\"Evaluate a machine learning model on four metrics:\n",
    "       ROC AUC, precision score, recall score, and f1 score.\n",
    "       \n",
    "       Returns the model and the predictions.\"\"\"\n",
    "    \n",
    "    model.fit(train, y)\n",
    "\n",
    "    # Predict probabilities and labels\n",
    "    probs = model.predict_proba(test)[:, 1]\n",
    "    preds = model.predict(test)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc = roc_auc_score(test_y, probs)\n",
    "    name = repr(model).split('(')[0]\n",
    "    print(f\"{name}\\n\")\n",
    "    print(f'ROC AUC: {round(roc, 4)}')\n",
    "\n",
    "    # Iterate through metrics\n",
    "    for metric in [precision_score, recall_score, f1_score]:\n",
    "        # Use .__name__ attribute to list metric\n",
    "        print(f'{metric.__name__}: {round(metric(test_y, preds), 4)}')\n",
    "\n",
    "    return model, preds\n",
    "\n",
    "\n",
    "model, preds = evaluate(model, train, y, test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model does perform slightly better than guessing in terms of ROC AUC. The precision is slightly higher although the recall is much lower leading to an overall reduced f1 score (with a default threshold of 0.5 for classifying positive examples). This poor performance by the logistic regression indicates the problem of separating churn from not-churn is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Model\n",
    "\n",
    "For a potentially better machine learning model, we can move to the [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). From the results of the logistic regression, this looks to be a non-linear problem which means we should use a model capable of learning a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use most of the default hyperparameters but alter a few to prevent overfitting. We can also set `class_weight = 'balanced'` to try and offset the impact of such an imbalanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.494Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=40,\n",
    "                               min_samples_leaf = 50,\n",
    "                               n_jobs=-1, class_weight = 'balanced',\n",
    "                               random_state = 50)\n",
    "\n",
    "model, preds = evaluate(model, train, y, test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest performance is much better than just guessing! With no tuning, the model is able to identify 36% of the customer churns and the false positives have been reduced. This should give us confidence that we can solve this problem using machine learning.\n",
    "\n",
    "# Model Validation\n",
    "\n",
    "We need to inspect the model results to determine if it meets our business needs. This includes looking at the performance as well as the feature importances. We want to make sure that our model performs well, but also try and understand _why_ it performs well. \n",
    "\n",
    "## Precision Recall Curve\n",
    "\n",
    "One of the best methods for tuning a model for a business need is through the precision recall curve. This shows the precision-recall tradeoff for different thresholds. Depending on the business requirement, we can change the threshold for classifying a positive example to alter the balance of true positives, false positives, false negatives, and true negatives. There will always be a tradeoff between precision and recall, but we can try to find the right balance by visually and quantitatively assessing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.498Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn');\n",
    "\n",
    "def plot_precision_recall(test_y, probs, title = 'Precision Recall Curve'):\n",
    "    \"\"\"Plot a precision recall curve for predictions. \n",
    "       Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\"\"\"\n",
    "    \n",
    "    precision, recall, threshold = precision_recall_curve(test_y, probs)\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall', size = 18)\n",
    "    plt.ylabel('Precision', size = 18)\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(title, size = 20)\n",
    "    plt.xticks(size = 14); plt.yticks(size = 14)\n",
    "    \n",
    "    pr = pd.DataFrame({'precision': precision[:-1], 'recall': recall[:-1],\n",
    "                       'threshold': threshold})\n",
    "    return pr\n",
    "    \n",
    "probs = model.predict_proba(test)[:, 1]\n",
    "pr_data = plot_precision_recall(test_y, probs, title = 'Precision-Recall Curve for Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the dataframe to find the threshold required for a given precision or recall. For example, to find the threshold for a precision of 50%, we use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.503Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_above = pr_data.loc[pr_data['precision'] >= 0.5].copy()\n",
    "precision_above.sort_values('recall', ascending = False, inplace = True)\n",
    "precision_above.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this threshold, our model must output a probability greater than 0.95 for an example to be classified as a positive churn. With this value, our model only identifies 0.11% of the actual churns as shown by the recall.\n",
    "\n",
    "### Adjusting for the Business Requirement\n",
    "\n",
    "Let's say we are required to have a recall of 50% in our model. This means our model finds 50% of the true churns in the data. We'll work through the rest of this notebook under this assumption. To find the threshold, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.507Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_above = pr_data.loc[pr_data['recall'] >= 0.5].copy()\n",
    "recall_above.sort_values('precision', ascending = False, inplace = True)\n",
    "recall_above.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.512Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_attained = recall_above.iloc[0, 0]\n",
    "threshold_required = recall_above.iloc[0, -1]\n",
    "\n",
    "print(f'At a threshold of {round(threshold_required, 4)} the recall is 50% and the precision is {round(100 * precision_attained, 4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that in order to identify 50% of the actual churns, we'll have to accept that only 3.7% of the predicted positives are actually positive churns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is often a useful way to visualize predictions. This shows the true values along the top row and the predicted values along the bottom row. Looking at the different cells, we can see where the model performed well and where it did not do so well.\n",
    "\n",
    "We'll use the threshold identified above to construct the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.516Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.YlOrRd):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.style.use('bmh')\n",
    "    plt.figure(figsize = (9, 9))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 22)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 20)\n",
    "    plt.yticks(tick_marks, classes, size = 20)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                 size = 20)\n",
    "    plt.grid(None)\n",
    "    plt.ylabel('True label', size = 22)\n",
    "    plt.xlabel('Predicted label', size = 22)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions where probability is above threshold \n",
    "preds = np.zeros(len(test_y))\n",
    "preds[probs >= threshold_required] = 1\n",
    "\n",
    "# Make and plot confusion matrix\n",
    "cm = confusion_matrix(test_y, preds)\n",
    "plot_confusion_matrix(cm, classes = ['No Churn', 'Churn'],\n",
    "                      title = 'Churn Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our business requirement, this is the best prediction of what our performance would be on new data. The model is able to identiy 50% of churned customers compared to a baseline of around 3%. The precision has increased from the baseline 0.9% to 3.7%, a relative increase of over 300%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.523Z"
    }
   },
   "outputs": [],
   "source": [
    "abs(0.0087 - 0.037) / 0.0087"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "\n",
    "As one method for trying to understand how the model makes decisions, we can look at the most important features. The absolute value of the importances is not as useful as is the relative ranking of the features which is determined by how well the feature separates the classes when building the decision trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.525Z"
    }
   },
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'importance': model.feature_importances_}, index=train.columns).\\\n",
    "    sort_values('importance', ascending=False)\n",
    "fi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.529Z"
    }
   },
   "outputs": [],
   "source": [
    "fi.iloc[:10]['importance'].plot.barh(color = 'r', edgecolor = 'k', \n",
    "                                     figsize = (14, 10), linewidth = 2)\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis();\n",
    "plt.xticks(size = 20);\n",
    "plt.yticks(size = 18)\n",
    "plt.title('Most Important Features', size = 28);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why these features are important, we can plot the distribution colored by the value of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.532Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Iterate through features\n",
    "for feature in ['TIME_SINCE_LAST(transactions.transaction_date)',\n",
    "                'AVG_TIME_BETWEEN(transactions.transaction_date)',\n",
    "                'TOTAL_PREVIOUS_MONTH(transactions.daily_price, membership_expire_date)',\n",
    "                'TOTAL_PREVIOUS_MONTH(transactions.actual_amount_paid, membership_expire_date)']:\n",
    "    plt.figure(figsize = (16, 8))\n",
    "    \n",
    "    # Iterate through values of the label\n",
    "    for label, grouped in train.groupby(y):\n",
    "        # Plot the distribution of the feature\n",
    "        sns.kdeplot(grouped[feature].dropna(), \n",
    "                    label = 'Churned' if label == 1 else 'No Churn');\n",
    "    # Plot labeling\n",
    "    plt.ylabel('Density'); plt.xlabel(f'{feature.capitalize()}')\n",
    "    plt.title(f'Distribution of {feature.capitalize()}', size = 28);\n",
    "    plt.legend(prop= {'size': 20});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are significant differences between those customers who churned and those who did not. Most noticeably, the customers who churned had less activity in the previous month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Model Optimization Using TPOT\n",
    "\n",
    "If we are not pleased with the model results using an off-the-shelf algorithm from Scikit-Learn, there are a number of libraries for searching for the best model without any manual intervention. One of the easiest-to-se libraries is known as TPOT. This will search through hundreds of machine learning models, using evolutionary algorithms to guide the discovery process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the models, we'll use a TimeSeriesSplit. This makes three splits of the data based on the indexes so we need to ensure that our data is sorted by time (already done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.535Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a `TPOTClassifier` object and pass in a few parameters. This particular use case will search 100 models, using `f1` scoring, the TimeSeriesSplit for cross validation, and taking advantage of all the cores on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.537Z"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Make tpot pipeline\n",
    "tpot_pipeline = TPOTClassifier(generations = 10, population_size = 10, \n",
    "                               cv = tss, scoring = 'f1', \n",
    "                               n_jobs = -1, verbosity = 2,\n",
    "                               random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.541Z"
    }
   },
   "outputs": [],
   "source": [
    "tpot_pipeline.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.543Z"
    }
   },
   "outputs": [],
   "source": [
    "tpot_pipeline.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-24T00:57:03.545Z"
    }
   },
   "outputs": [],
   "source": [
    "tpot_pipeline.export('best_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pipeline\n",
    "\n",
    "The following code shows the best pipeline exported by TPOT. We can just copy the code, train a model, and make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:0.4381\n",
    "exported_pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    GradientBoostingClassifier(learning_rate=0.01, max_depth=7, max_features=0.75, min_samples_leaf=12, min_samples_split=3, n_estimators=100, subsample=0.05)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T22:57:08.389915Z",
     "start_time": "2018-10-23T22:57:08.387753Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Average CV score on the training set was:0.43811110082970917\n",
    "# exported_pipeline = make_pipeline(\n",
    "#     MinMaxScaler(),\n",
    "#     GradientBoostingClassifier(learning_rate=0.01, max_depth=7, max_features=0.75, \n",
    "#                                min_samples_leaf=12, min_samples_split=3, n_estimators=100, subsample=0.05)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T22:57:08.651864Z",
     "start_time": "2018-10-23T22:57:08.391225Z"
    }
   },
   "outputs": [],
   "source": [
    "model, preds = evaluate(exported_pipeline, train, y, test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T22:57:08.652486Z",
     "start_time": "2018-10-23T22:04:15.101Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_y, preds)\n",
    "plot_confusion_matrix(cm, classes = ['No Churn', 'Churn'], title = 'Optimized Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T22:57:08.653142Z",
     "start_time": "2018-10-23T22:04:15.103Z"
    }
   },
   "outputs": [],
   "source": [
    "probs = model.predict_proba(test)[:, 1]\n",
    "plot_precision_recall(test_y, probs, title = 'Precision-Recall Curve for Optimized Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
